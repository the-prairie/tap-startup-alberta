{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas \n",
    "import json\n",
    "from flatten_json import flatten\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAP_STARTUP_ALBERTA_TOKEN = os.getenv(\"TAP_STARTUP_ALBERTA_TOKEN\")\n",
    "TAP_STARTUP_ALBERTA_APP_ID = os.getenv(\"TAP_STARTUP_ALBERTA_APP_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the purpose of this module is to convert JSON schema to BigQuery schema.\n",
    "\"\"\"\n",
    "\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import re\n",
    "\n",
    "METADATA_FIELDS = {\n",
    "    \"_time_extracted\": {\"type\": [\"null\", \"string\"], \"format\": \"date-time\", \"bq_type\": \"timestamp\"},\n",
    "    \"_time_loaded\": {\"type\": [\"null\", \"string\"], \"format\": \"date-time\", \"bq_type\": \"timestamp\"}\n",
    "}\n",
    "\n",
    "\n",
    "def cleanup_record(schema, record):\n",
    "    \"\"\"\n",
    "    Clean up / prettify field names, make sure they match BigQuery naming conventions.\n",
    "\n",
    "    :param JSON record generated by the tap and piped into target-bigquery\n",
    "    :param bq_schema: JSON schema generated by the tap and piped into target-bigquery\n",
    "    :return: JSON record/data, where field names are cleaned up / prettified.\n",
    "    \"\"\"\n",
    "    if not isinstance(record, dict) and not isinstance(record, list):\n",
    "        return record\n",
    "\n",
    "    elif isinstance(record, list):\n",
    "        nr = []\n",
    "        for item in record:\n",
    "            nr.append(cleanup_record(schema, item))\n",
    "        return nr\n",
    "\n",
    "    elif isinstance(record, dict):\n",
    "        nr = {}\n",
    "        for key, value in record.items():\n",
    "            nkey = create_valid_bigquery_field_name(key)\n",
    "            nr[nkey] = cleanup_record(schema, value)\n",
    "        return nr\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"unhandled instance of record: {record}\")\n",
    "\n",
    "\n",
    "def create_valid_bigquery_field_name(field_name):\n",
    "    \"\"\"\n",
    "    Clean up / prettify field names, make sure they match BigQuery naming conventions.\n",
    "    \n",
    "    Fields must:\n",
    "        • contain only \n",
    "            -letters, \n",
    "            -numbers, and \n",
    "            -underscores, \n",
    "        • start with a \n",
    "            -letter or \n",
    "            -underscore, and\n",
    "        • be at most 300 characters long\n",
    "\n",
    "    :param key: JSON field name\n",
    "    :return: cleaned up JSON field name\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_up_field_name = \"\"\n",
    "\n",
    "    # if char is alphanumeric (either letters or numbers), append char to our string\n",
    "    for char in field_name:\n",
    "        if char.isalnum():\n",
    "            cleaned_up_field_name += char\n",
    "        else:\n",
    "            # otherwise, replace it with underscore\n",
    "            cleaned_up_field_name += \"_\"\n",
    "\n",
    "    # if field starts with digit, prepend it with underscore\n",
    "    if cleaned_up_field_name[0].isdigit():\n",
    "        cleaned_up_field_name = \"_%s\" % cleaned_up_field_name\n",
    "\n",
    "    return cleaned_up_field_name[:300] # trim the string to the first x chars\n",
    "\n",
    "def prioritize_one_data_type_from_multiple_ones_in_any_of(field_property):\n",
    "    \"\"\"\n",
    "    :param field_property: JSON field property, which has anyOf and multiple data types\n",
    "    :return: one BigQuery SchemaField field_type, which is prioritized\n",
    "\n",
    "    Simplification step removes anyOf columns from original JSON schema.\n",
    "\n",
    "    There's one instance when original JSON schema has no anyOf, but anyOf gets added:\n",
    "\n",
    "    original JSON schema:\n",
    "\n",
    "     \"simplification_stage_adds_anyOf\": {\n",
    "      \"type\": [\n",
    "        \"null\",\n",
    "        \"integer\",\n",
    "        \"string\"\n",
    "      ]\n",
    "    }\n",
    "\n",
    "     This is a simplified JSON schema where anyOf got added during\n",
    "     simplification stage:\n",
    "\n",
    "      {'simplification_stage_added_anyOf': {\n",
    "            'anyOf': [\n",
    "                {\n",
    "                    'type': [\n",
    "                        'integer',\n",
    "                        'null'\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'type': [\n",
    "                        'string',\n",
    "                        'null'\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        }\n",
    "\n",
    "    The VALUE of this dictionary will be the INPUT for this function.\n",
    "\n",
    "    This simplified case needs to be handled.\n",
    "\n",
    "    Prioritization needs to be applied:\n",
    "        1) STRING\n",
    "        2) FLOAT\n",
    "        3) INTEGER\n",
    "        4) BOOLEAN\n",
    "\n",
    "    OUTPUT of the function is one JSON data type with the top priority\n",
    "    \"\"\"\n",
    "\n",
    "    prioritization_dict = {\"string\": 1,\n",
    "                           \"number\": 2,\n",
    "                           \"integer\": 3,\n",
    "                           \"boolean\": 4,\n",
    "                           \"object\": 5,\n",
    "                           \"array\": 6,\n",
    "                           }\n",
    "\n",
    "    any_of_data_types = {}\n",
    "\n",
    "    for i in range(0, len(field_property['anyOf'])):\n",
    "        data_type = field_property['anyOf'][i]['type'][0]\n",
    "\n",
    "        any_of_data_types.update({data_type: prioritization_dict[data_type]})\n",
    "\n",
    "    # return key with minimum value, which is the highest priority data type\n",
    "    # https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "    return min(any_of_data_types, key=any_of_data_types.get)\n",
    "\n",
    "\n",
    "def convert_field_type(field_property):\n",
    "    \"\"\"\n",
    "    :param field_property: JSON field property\n",
    "    :return: BigQuery SchemaField field_type\n",
    "    \"\"\"\n",
    "\n",
    "    conversion_dict = {\"string\": \"STRING\",\n",
    "                       \"number\": \"FLOAT\",\n",
    "                       \"integer\": \"INTEGER\",\n",
    "                       \"boolean\": \"BOOLEAN\",\n",
    "                       \"date-time\": \"TIMESTAMP\",\n",
    "                       \"date\": \"DATE\",\n",
    "                       \"time\": \"TIME\",\n",
    "                       \"object\": \"RECORD\",\n",
    "                       \"array\": \"RECORD\",\n",
    "                       \"bq-geography\": \"GEOGRAPHY\",\n",
    "                       \"bq-decimal\": \"DECIMAL\",\n",
    "                       \"bq-bigdecimal\": \"BIGDECIMAL\"\n",
    "                       }\n",
    "\n",
    "    if \"anyOf\" in field_property:\n",
    "\n",
    "        prioritized_data_type = prioritize_one_data_type_from_multiple_ones_in_any_of(field_property)\n",
    "\n",
    "        field_type_bigquery = conversion_dict[prioritized_data_type]\n",
    "\n",
    "    elif field_property[\"type\"][0] == \"string\" and \"format\" in field_property:\n",
    "\n",
    "        field_type_bigquery = conversion_dict[field_property[\"format\"]]\n",
    "\n",
    "    elif ((\"items\" in field_property) and (\"properties\" not in field_property[\"items\"])):\n",
    "\n",
    "        field_type_bigquery = conversion_dict[field_property['items']['type'][0]]\n",
    "\n",
    "    else:\n",
    "\n",
    "        field_type_bigquery = conversion_dict[field_property[\"type\"][0]]\n",
    "\n",
    "    return field_type_bigquery\n",
    "\n",
    "\n",
    "def determine_field_mode(field_name, field_property):\n",
    "    \"\"\"\n",
    "    :param field_name: one nested JSON field name\n",
    "    :param field_property: one nested JSON field property\n",
    "    :return: BigQuery SchemaField mode\n",
    "    \"\"\"\n",
    "    if \"items\" in field_property:\n",
    "\n",
    "        field_mode = 'REPEATED'\n",
    "\n",
    "    else:\n",
    "\n",
    "        field_mode = 'NULLABLE'\n",
    "\n",
    "    return field_mode\n",
    "\n",
    "\n",
    "def replace_nullable_mode_with_required(schema_field_input):\n",
    "    schema_field_updated = SchemaField(name=schema_field_input.name,\n",
    "                                       field_type=schema_field_input.field_type,\n",
    "                                       mode='REQUIRED',\n",
    "                                       description=schema_field_input.description,\n",
    "                                       fields=schema_field_input.fields,\n",
    "                                       policy_tags=schema_field_input.policy_tags)\n",
    "\n",
    "    return schema_field_updated\n",
    "\n",
    "\n",
    "def build_field(field_name, field_property):\n",
    "    \"\"\"\n",
    "    :param field_name: one nested JSON field name\n",
    "    :param field_property: one nested JSON field property\n",
    "    :return: one BigQuery nested SchemaField\n",
    "    \"\"\"\n",
    "\n",
    "    if not (\"items\" in field_property and \"properties\" in field_property[\"items\"]) and not (\n",
    "            \"properties\" in field_property):\n",
    "\n",
    "        return (SchemaField(name=create_valid_bigquery_field_name(field_name),\n",
    "                            field_type=convert_field_type(field_property),\n",
    "                            mode=determine_field_mode(field_name, field_property),\n",
    "                            description=None,\n",
    "                            fields=(),\n",
    "                            policy_tags=None)\n",
    "                )\n",
    "\n",
    "    elif (\"items\" in field_property and \"properties\" in field_property[\"items\"]) or (\"properties\" in field_property):\n",
    "\n",
    "        processed_subfields = []\n",
    "\n",
    "        # https://www.w3schools.com/python/ref_dictionary_get.asp\n",
    "        for subfield_name, subfield_property in field_property.get(\"properties\",\n",
    "                                                                   field_property.get(\"items\", {}).get(\"properties\")\n",
    "                                                                   ).items():\n",
    "            processed_subfields.append(build_field(subfield_name, subfield_property))\n",
    "\n",
    "        return (SchemaField(name=create_valid_bigquery_field_name(field_name),\n",
    "                            field_type=convert_field_type(field_property),\n",
    "                            mode=determine_field_mode(field_name, field_property),\n",
    "                            description=None,\n",
    "                            fields=processed_subfields,\n",
    "                            policy_tags=None)\n",
    "                )\n",
    "\n",
    "\n",
    "def build_schema(schema, key_properties=None, add_metadata=True, force_fields={}):\n",
    "    \"\"\"\n",
    "    :param schema: input simplified JSON schema\n",
    "    :param key_properties: JSON schema fields which will become required BigQuery column\n",
    "    :param add_metadata: do we want BigQuery metadata columns (e.g., when data was uploaded?)\n",
    "    :param force_fields: You can force a field to a desired data type via force_fields flag.\n",
    "            Use case example:\n",
    "            tap facebook field \"date_start\" from stream ads_insights_age_and_gender is being passed as string to BQ,\n",
    "            which contradicts tap catalog file, where we said it's a date. force_fields fixes this issue.\n",
    "    :return: a list of BigQuery SchemaFields, which represents one BigQuery table\n",
    "    \"\"\"\n",
    "\n",
    "    global required_fields\n",
    "\n",
    "    required_fields = set(key_properties) if key_properties else set()\n",
    "\n",
    "    schema_bigquery = []\n",
    "\n",
    "    for field_name, field_property in schema.get(\"properties\", schema.get(\"items\", {}).get(\"properties\", {})).items():\n",
    "\n",
    "        if field_name in force_fields:\n",
    "\n",
    "            next_field = (\n",
    "                SchemaField(field_name, force_fields[field_name][\"type\"],\n",
    "                            force_fields[field_name].get(\"mode\", \"nullable\"),\n",
    "                            force_fields[field_name].get(\"description\", None), ())\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            next_field = build_field(field_name, field_property)\n",
    "\n",
    "            if field_name in required_fields:\n",
    "                next_field = replace_nullable_mode_with_required(next_field)\n",
    "\n",
    "        schema_bigquery.append(next_field)\n",
    "\n",
    "    if add_metadata:\n",
    "\n",
    "        for field_name in METADATA_FIELDS:\n",
    "            schema_bigquery.append(SchemaField(name=field_name,\n",
    "                                               field_type=METADATA_FIELDS[field_name][\"bq_type\"],\n",
    "                                               mode='NULLABLE',\n",
    "                                               description=None,\n",
    "                                               fields=(),\n",
    "                                               policy_tags=None)\n",
    "                                   )\n",
    "\n",
    "    return schema_bigquery\n",
    "\n",
    "\n",
    "def format_record_to_schema(record, bq_schema):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Singer tap outputs two things: JSON schema and JSON record/data.\n",
    "        Sometimes tap outputs data, where type doesn't match schema produced by the tap.\n",
    "        This function makes sure that the data matches the schema.\n",
    "\n",
    "    RECORD is not included into conversion_dict - it is done on purpose. RECORD is handled recursively.\n",
    "\n",
    "    :param JSON record generated by the tap and piped into target-bigquery\n",
    "    :param bq_schema: JSON schema generated by the tap and piped into target-bigquery\n",
    "    :return: JSON record/data, where the data types match JSON schema\n",
    "    \"\"\"\n",
    "\n",
    "    conversion_dict = {\"BYTES\": bytes,\n",
    "                       \"STRING\": str,\n",
    "                       \"TIME\": str,\n",
    "                       \"TIMESTAMP\": str,\n",
    "                       \"DATE\": str,\n",
    "                       \"DATETIME\": str,\n",
    "                       \"FLOAT\": float,\n",
    "                       \"NUMERIC\": float,\n",
    "                       \"BIGNUMERIC\": float,\n",
    "                       \"INTEGER\": int,\n",
    "                       \"BOOLEAN\": bool,\n",
    "                       \"GEOGRAPHY\": str,\n",
    "                       \"DECIMAL\": str,\n",
    "                       \"BIGDECIMAL\": str\n",
    "                       }\n",
    "\n",
    "    if isinstance(record, list):\n",
    "        new_record = []\n",
    "        for r in record:\n",
    "            if isinstance(r, dict):\n",
    "                r = format_record_to_schema(r, bq_schema)\n",
    "                new_record.append(r)\n",
    "            else:\n",
    "                raise Exception(f\"unhandled instance of list object in record: {r}\")\n",
    "        return new_record\n",
    "    elif isinstance(record, dict):\n",
    "        rc = record.copy()\n",
    "        for k, v in rc.items():\n",
    "            if k not in bq_schema:\n",
    "                record.pop(k)\n",
    "            elif v is None:\n",
    "                pass\n",
    "            elif bq_schema[k].get(\"fields\"):\n",
    "                # mode: REPEATED, type: NULLABLE || mode: REPEATED: type: REPEATED\n",
    "                record[k] = format_record_to_schema(record[k], bq_schema[k][\"fields\"])\n",
    "            elif bq_schema[k].get(\"mode\") == \"REPEATED\":\n",
    "                # mode: REPEATED, type: [any]\n",
    "                record[k] = [conversion_dict[bq_schema[k][\"type\"]](vi) for vi in v]\n",
    "            else:\n",
    "                record[k] = conversion_dict[bq_schema[k][\"type\"]](v)\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_bq_schema_dict(schema):  # could move this to derived class but seems right to handle in base\n",
    "        \"\"\"\n",
    "        Convert BigQuery schema as a list to BigQuery schema as a dictionary\n",
    "        :param schema, list of BigQuery SchemaFields\n",
    "        :return: schema_dict, dict. Dict of BigQuery schema fields.\n",
    "            Dict key is field name\n",
    "            Dict value is a dict also. It has BigQuery mode and type\n",
    "        \"\"\"\n",
    "        schema_dict = {}\n",
    "        for field in schema:\n",
    "            f = field if isinstance(field, dict) else field.to_api_repr()\n",
    "            schema_dict[f[\"name\"]] = f\n",
    "            if f.get(\"fields\"):\n",
    "                schema_dict[f[\"name\"]][\"fields\"] = _build_bq_schema_dict(f[\"fields\"])\n",
    "            schema_dict[f[\"name\"]].pop(\"description\")\n",
    "            schema_dict[f[\"name\"]].pop(\"name\")\n",
    "        return schema_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/zar/Desktop/modern-data-stack/meltano_taps/tap-startup-alberta/tap_startup_alberta/schemas/companies.json\") as f:\n",
    "        companies_schema = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetriableAPIError(Exception):\n",
    "    \"\"\"Exception raised when a failed request can be safely retried.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional, Union, List, Iterable, Callable\n",
    "from singer.schema import Schema\n",
    "from memoization import cached\n",
    "import backoff\n",
    "\n",
    "from singer_sdk.helpers.jsonpath import extract_jsonpath\n",
    "from singer_sdk.streams import RESTStream\n",
    "from singer_sdk.authenticators import BasicAuthenticator\n",
    "from singer_sdk.authenticators import APIAuthenticatorBase, SimpleAuthenticator\n",
    "from singer_sdk.helpers.jsonpath import extract_jsonpath\n",
    "\n",
    "\n",
    "class StartupStream():\n",
    "    \"\"\"startup-alberta stream class.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Optional[Union[Dict[str, Any], Schema]] = None\n",
    "    ) -> None:\n",
    "        \n",
    "        \"\"\"Initialize the REST stream.\n",
    "\n",
    "        Args:\n",
    "            tap: Singer Tap this stream belongs to.\n",
    "            schema: JSON schema for records in this stream.\n",
    "            name: Name of this stream.\n",
    "            path: URL path for this entity stream.\n",
    "        \"\"\"\n",
    "        self.schema = schema\n",
    "        self.path = \"/companies\"\n",
    "        self.config = {\"token\": TAP_STARTUP_ALBERTA_TOKEN, \"app_id\": TAP_STARTUP_ALBERTA_APP_ID}\n",
    "\n",
    "        self._http_headers: dict = {}\n",
    "        self._requests_session = requests.Session()\n",
    "        self._compiled_jsonpath = None\n",
    "        self._next_page_token_compiled_jsonpath = None\n",
    "        \n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _url_encode(val: Union[str, datetime, bool, int, List[str]]) -> str:\n",
    "        \"\"\"Encode the val argument as url-compatible string.\n",
    "\n",
    "        Args:\n",
    "            val: TODO\n",
    "\n",
    "        Returns:\n",
    "            TODO\n",
    "        \"\"\"\n",
    "        if isinstance(val, str):\n",
    "            result = val.replace(\"/\", \"%2F\")\n",
    "        else:\n",
    "            result = str(val)\n",
    "        return result\n",
    "\n",
    "    def get_url(self, context: Optional[dict]) -> str:\n",
    "        \"\"\"Get stream entity URL.\n",
    "\n",
    "        Developers override this method to perform dynamic URL generation.\n",
    "\n",
    "        Args:\n",
    "            context: Stream partition or context dictionary.\n",
    "\n",
    "        Returns:\n",
    "            A URL, optionally targeted to a specific partition or context.\n",
    "        \"\"\"\n",
    "        url = \"\".join([self.url_base, self.path or \"\"])\n",
    "        vals = copy.copy(dict(self.config))\n",
    "        vals.update(context or {})\n",
    "        for k, v in vals.items():\n",
    "            search_text = \"\".join([\"{\", k, \"}\"])\n",
    "            if search_text in url:\n",
    "                url = url.replace(search_text, self._url_encode(v))\n",
    "        return url\n",
    "\n",
    "    \n",
    "    rest_method = \"POST\"\n",
    "\n",
    "    url_base = \"https://api.dealroom.co/api/v2\"\n",
    "\n",
    "    records_jsonpath = \"$.items[*]\"  # Or override `parse_response`.\n",
    "    \n",
    "    max_offset = 25\n",
    "    # Private constants. May not be supported in future releases:\n",
    "    _LOG_REQUEST_METRICS: bool = True\n",
    "    # Disabled by default for safety:\n",
    "    _LOG_REQUEST_METRIC_URLS: bool = False\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def http_headers(self) -> dict:\n",
    "        \"\"\"Return the http headers needed.\"\"\"\n",
    "        headers = {\n",
    "            \n",
    "            \"authority\": \"api.dealroom.co\",\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"origin\": \"https://ecosystem.startalberta.ca\",\n",
    "            \"x-dealroom-app-id\": TAP_STARTUP_ALBERTA_APP_ID,\n",
    "            \"x-requested-with\": \"XMLHttpRequest\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br\"\n",
    "            \n",
    "        }\n",
    "\n",
    "        return headers\n",
    "\n",
    "\n",
    "    def get_url_params(\n",
    "        self, context: Optional[dict], next_page_token: Optional[Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        \"\"\"Return a dictionary of values to be used in URL parameterization.\"\"\"\n",
    "        params: dict = {}\n",
    "        params[\"token\"] = TAP_STARTUP_ALBERTA_TOKEN\n",
    "        return params\n",
    "\n",
    "    def prepare_request_payload(\n",
    "        self, context: Optional[dict], next_page_token: Optional[Any]\n",
    "    ) -> Optional[dict]:\n",
    "        \n",
    "\n",
    "        \"\"\"Prepare the data payload for the REST API request.\n",
    "\n",
    "        By default, no payload will be sent (return None).\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"fields\":\"id,angellist_url,appstore_app_id,client_focus,company_status,core_side_value,corporate_industries,create_date,crunchbase_url,employee_12_months_growth_delta,employee_12_months_growth_percentile,employee_12_months_growth_relative,employee_12_months_growth_unique,employee_3_months_growth_delta,employee_3_months_growth_percentile,employee_3_months_growth_relative,employee_3_months_growth_unique,employee_6_months_growth_delta,employee_6_months_growth_percentile,employee_6_months_growth_relative,employee_6_months_growth_unique,employees_chart,employees_latest,employees,entity_sub_types,facebook_url,founders_score_cumulated,founders,founders_top_university,founders_top_past_companies,fundings,fundings,growth_stage,has_strong_founder,has_super_founder,has_promising_founder,hq_locations,images,income_streams,industries,innovations,innovations_count,investments,investors,is_editorial,is_ai_data,is_from_traderegister,latest_revenue_enhanced,latest_valuation_enhanced,launch_month,launch_year,linkedin_url,lists_ids,matching_score,name,participated_events,past_founders_raised_10m,past_founders,path,playmarket_app_id,revenues,sdgs,service_industries,similarweb_12_months_growth_delta,similarweb_12_months_growth_percentile,similarweb_12_months_growth_relative,similarweb_12_months_growth_unique,similarweb_3_months_growth_delta,similarweb_3_months_growth_percentile,similarweb_3_months_growth_relative,similarweb_3_months_growth_unique,similarweb_6_months_growth_delta,similarweb_6_months_growth_percentile,similarweb_6_months_growth_relative,similarweb_6_months_growth_unique,similarweb_chart,sub_industries,tags,tagline,technologies,total_funding,total_jobs_available,trading_multiples,type,tech_stack,twitter_url,job_roles\",\n",
    "            \"limit\":25,\n",
    "            \"offset\": next_page_token,\n",
    "            \"form_data\": \n",
    "             {\"must\":{\"filters\":{\"all_slug_locations\":{\"values\":[\"alberta_1\"],\"execution\":\"and\"}},\"execution\":\"and\"},\n",
    "              \"should\":{\"filters\":{}},\n",
    "              \"must_not\":{\"growth_stages\":[\"mature\"],\"company_type\":[\"service provider\",\"government nonprofit\"],\"tags\":[\"outside tech\"],\"company_status\":[\"closed\"]}\n",
    "              },\n",
    "         \"sort\":\"-last_funding_date\"}\n",
    "        \n",
    "        return payload\n",
    "    \n",
    "    def get_next_page_token(\n",
    "        self, response: requests.Response, previous_token: Optional[Any],\n",
    "    ) -> Optional[Any]:\n",
    "        \"\"\"Return token identifying next page or None if all records have been read.\n",
    "\n",
    "        Args:\n",
    "            response: A raw `requests.Response`_ object.\n",
    "            previous_token: Previous pagination reference.\n",
    "\n",
    "        Returns:\n",
    "            Reference value to retrieve next page.\n",
    "\n",
    "        .. _requests.Response:\n",
    "            https://docs.python-requests.org/en/latest/api/#requests.Response\n",
    "        \"\"\"\n",
    "        previous_token = json.loads(response.request.body.decode())[\"offset\"] or 0\n",
    "        total_items = response.json()['total']\n",
    "        \n",
    "        if previous_token < total_items:\n",
    "            return previous_token + self.max_offset\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def requests_session(self) -> requests.Session:\n",
    "        \"\"\"Get requests session.\n",
    "\n",
    "        Returns:\n",
    "            The `requests.Session`_ object for HTTP requests.\n",
    "\n",
    "        .. _requests.Session:\n",
    "            https://docs.python-requests.org/en/latest/api/#request-sessions\n",
    "        \"\"\"\n",
    "        if not self._requests_session:\n",
    "            self._requests_session = requests.Session()\n",
    "        return self._requests_session\n",
    "\n",
    "    def validate_response(self, response: requests.Response) -> None:\n",
    "        \"\"\"Validate HTTP response.\n",
    "\n",
    "        By default, checks for error status codes (>400) and raises a\n",
    "        :class:`singer_sdk.exceptions.FatalAPIError`.\n",
    "\n",
    "        Tap developers are encouraged to override this method if their APIs use HTTP\n",
    "        status codes in non-conventional ways, or if they communicate errors\n",
    "        differently (e.g. in the response body).\n",
    "\n",
    "        .. image:: ../images/200.png\n",
    "\n",
    "\n",
    "        In case an error is deemed transient and can be safely retried, then this\n",
    "        method should raise an :class:`singer_sdk.exceptions.RetriableAPIError`.\n",
    "\n",
    "        Args:\n",
    "            response: A `requests.Response`_ object.\n",
    "\n",
    "        Raises:\n",
    "            FatalAPIError: If the request is not retriable.\n",
    "            RetriableAPIError: If the request is retriable.\n",
    "\n",
    "        .. _requests.Response:\n",
    "            https://docs.python-requests.org/en/latest/api/#requests.Response\n",
    "        \"\"\"\n",
    "        if 400 <= response.status_code < 500:\n",
    "            msg = (\n",
    "                f\"{response.status_code} Client Error: \"\n",
    "                f\"{response.reason} for path: {self.path}\"\n",
    "            )\n",
    "            raise msg\n",
    "\n",
    "        elif 500 <= response.status_code < 600:\n",
    "            msg = (\n",
    "                f\"{response.status_code} Server Error: \"\n",
    "                f\"{response.reason} for path: {self.path}\"\n",
    "            )\n",
    "            raise msg\n",
    "\n",
    "    def request_decorator(self, func: Callable) -> Callable:\n",
    "        \"\"\"Instantiate a decorator for handling request failures.\n",
    "\n",
    "        Developers may override this method to provide custom backoff or retry\n",
    "        handling.\n",
    "\n",
    "        Args:\n",
    "            func: Function to decorate.\n",
    "\n",
    "        Returns:\n",
    "            A decorated method.\n",
    "        \"\"\"\n",
    "        decorator: Callable = backoff.on_exception(\n",
    "            backoff.expo,\n",
    "            (RetriableAPIError,),\n",
    "            max_tries=5,\n",
    "            factor=2,\n",
    "        )(func)\n",
    "        return decorator\n",
    "\n",
    "    def _request(\n",
    "        self, prepared_request: requests.PreparedRequest, context: Optional[dict]\n",
    "    ) -> requests.Response:\n",
    "        \"\"\"TODO.\n",
    "\n",
    "        Args:\n",
    "            prepared_request: TODO\n",
    "            context: Stream partition or context dictionary.\n",
    "\n",
    "        Returns:\n",
    "            TODO\n",
    "        \"\"\"\n",
    "        response = self.requests_session.send(prepared_request)\n",
    "        \n",
    "        self.validate_response(response)\n",
    "        logging.debug(\"Response received successfully.\")\n",
    "        return response\n",
    "    \n",
    "    def request_records(self, context: Optional[dict]) -> Iterable[dict]:\n",
    "        \"\"\"Request records from REST endpoint(s), returning response records.\n",
    "\n",
    "        If pagination is detected, pages will be recursed automatically.\n",
    "\n",
    "        Args:\n",
    "            context: Stream partition or context dictionary.\n",
    "\n",
    "        Yields:\n",
    "            An item for every record in the response.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If a loop in pagination is detected. That is, when two\n",
    "                consecutive pagination tokens are identical.\n",
    "        \"\"\"\n",
    "        next_page_token: Any = None\n",
    "        finished = False\n",
    "        decorated_request = self.request_decorator(self._request)\n",
    "\n",
    "        while not finished:\n",
    "            prepared_request = self.prepare_request(\n",
    "                context, next_page_token=next_page_token\n",
    "            )\n",
    "            resp = decorated_request(prepared_request, context)\n",
    "            for row in self.parse_response(resp):\n",
    "                yield row\n",
    "            previous_token = copy.deepcopy(next_page_token)\n",
    "            next_page_token = self.get_next_page_token(\n",
    "                response=resp, previous_token=previous_token\n",
    "            )\n",
    "            if next_page_token and next_page_token == previous_token:\n",
    "                raise RuntimeError(\n",
    "                    f\"Loop detected in pagination. \"\n",
    "                    f\"Pagination token {next_page_token} is identical to prior token.\"\n",
    "                )\n",
    "            # Cycle until get_next_page_token() no longer returns a value\n",
    "            finished = not next_page_token\n",
    "            \n",
    "    def get_records(self, context: Optional[dict]) -> Iterable[Dict[str, Any]]:\n",
    "        \"\"\"Return a generator of row-type dictionary objects.\n",
    "\n",
    "        Each row emitted should be a dictionary of property names to their values.\n",
    "\n",
    "        Args:\n",
    "            context: Stream partition or context dictionary.\n",
    "\n",
    "        Yields:\n",
    "            One item per (possibly processed) record in the API.\n",
    "        \"\"\"\n",
    "        for record in self.request_records(context):\n",
    "            transformed_record = self.post_process(record, context)\n",
    "            if transformed_record is None:\n",
    "                # Record filtered out during post_process()\n",
    "                continue\n",
    "            yield transformed_record\n",
    "            \n",
    "    def prepare_request(\n",
    "        self, context: Optional[dict], next_page_token: Optional[Any]\n",
    "    ) -> requests.PreparedRequest:\n",
    "        \"\"\"Prepare a request object.\n",
    "\n",
    "        If partitioning is supported, the `context` object will contain the partition\n",
    "        definitions. Pagination information can be parsed from `next_page_token` if\n",
    "        `next_page_token` is not None.\n",
    "\n",
    "        Args:\n",
    "            context: Stream partition or context dictionary.\n",
    "            next_page_token: Token, page number or any request argument to request the\n",
    "                next page of data.\n",
    "\n",
    "        Returns:\n",
    "            Build a request with the stream's URL, path, query parameters,\n",
    "            HTTP headers and authenticator.\n",
    "        \"\"\"\n",
    "        http_method = self.rest_method\n",
    "        url: str = self.get_url(context)\n",
    "        params: dict = self.get_url_params(context, next_page_token)\n",
    "        request_data = self.prepare_request_payload(context, next_page_token)\n",
    "        headers = self.http_headers\n",
    "\n",
    "\n",
    "        request = self.requests_session.prepare_request(\n",
    "            requests.Request(\n",
    "                method=http_method,\n",
    "                url=url,\n",
    "                params=params,\n",
    "                headers=headers,\n",
    "                json=request_data,\n",
    "            ),\n",
    "        )\n",
    "        return request\n",
    "\n",
    "\n",
    "    def parse_response(self, response: requests.Response) -> Iterable[dict]:\n",
    "        \"\"\"Parse the response and return an iterator of result rows.\"\"\"\n",
    "        # TODO: Parse response body and return a set of records.\n",
    "        \n",
    "        yield from extract_jsonpath(self.records_jsonpath, input=response.json())\n",
    "\n",
    "    def post_process(self, row: dict, context: Optional[dict]) -> dict:\n",
    "        \"\"\"As needed, append or transform raw data to match expected structure.\"\"\"\n",
    "        row = flatten(row)\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "startup = StartupStream(schema=companies_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = startup.get_records(context={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3019447,\n",
       " 'angellist_url': None,\n",
       " 'appstore_app_id': None,\n",
       " 'client_focus_0_name': 'consumer',\n",
       " 'company_status': 'operational',\n",
       " 'core_side_value': None,\n",
       " 'corporate_industries': [],\n",
       " 'create_date': '2021-06-15T09:58:25+0100',\n",
       " 'crunchbase_url': None,\n",
       " 'employee_12_months_growth_delta': None,\n",
       " 'employee_12_months_growth_percentile': None,\n",
       " 'employee_12_months_growth_relative': None,\n",
       " 'employee_12_months_growth_unique': None,\n",
       " 'employee_3_months_growth_delta': None,\n",
       " 'employee_3_months_growth_percentile': None,\n",
       " 'employee_3_months_growth_relative': None,\n",
       " 'employee_3_months_growth_unique': None,\n",
       " 'employee_6_months_growth_delta': None,\n",
       " 'employee_6_months_growth_percentile': None,\n",
       " 'employee_6_months_growth_relative': None,\n",
       " 'employee_6_months_growth_unique': None,\n",
       " 'employees_chart_0_date': '2021-07-26',\n",
       " 'employees_chart_0_value': 1,\n",
       " 'employees_chart_1_date': '2021-10-14',\n",
       " 'employees_chart_1_value': 1,\n",
       " 'employees_latest': None,\n",
       " 'employees': '2-10',\n",
       " 'entity_sub_types': [],\n",
       " 'facebook_url': None,\n",
       " 'founders_score_cumulated': 16,\n",
       " 'founders_0_id': 3019930,\n",
       " 'founders_0_name': 'Brent L.',\n",
       " 'founders_top_university': None,\n",
       " 'founders_top_past_companies': None,\n",
       " 'fundings_items_0_id': 536303,\n",
       " 'fundings_items_0_year': 2021,\n",
       " 'fundings_items_0_month': 12,\n",
       " 'fundings_items_0_amount': 1.9,\n",
       " 'fundings_items_0_currency': 'CAD',\n",
       " 'fundings_items_0_round': 'SEED',\n",
       " 'fundings_items_0_investors': [],\n",
       " 'fundings_items_0_unknown_investors': [],\n",
       " 'fundings_items_0_is_undisclosed': False,\n",
       " 'fundings_items_0_news_source': None,\n",
       " 'fundings_items_0_valuation': None,\n",
       " 'fundings_items_0_valuation_generated_min': 7600000,\n",
       " 'fundings_items_0_valuation_generated_max': 11400000,\n",
       " 'fundings_items_1_id': 454539,\n",
       " 'fundings_items_1_year': 2020,\n",
       " 'fundings_items_1_month': 12,\n",
       " 'fundings_items_1_amount': 1.15,\n",
       " 'fundings_items_1_currency': 'CAD',\n",
       " 'fundings_items_1_round': 'SECONDARY',\n",
       " 'fundings_items_1_investors': [],\n",
       " 'fundings_items_1_unknown_investors': [],\n",
       " 'fundings_items_1_is_undisclosed': False,\n",
       " 'fundings_items_1_news_source': None,\n",
       " 'fundings_items_1_valuation': 10000000,\n",
       " 'fundings_items_1_valuation_generated_min': None,\n",
       " 'fundings_items_1_valuation_generated_max': None,\n",
       " 'fundings_items_2_id': 459764,\n",
       " 'fundings_items_2_year': 2019,\n",
       " 'fundings_items_2_month': 10,\n",
       " 'fundings_items_2_amount': 0.25,\n",
       " 'fundings_items_2_currency': 'CAD',\n",
       " 'fundings_items_2_round': 'SEED',\n",
       " 'fundings_items_2_investors': [],\n",
       " 'fundings_items_2_unknown_investors': [],\n",
       " 'fundings_items_2_is_undisclosed': False,\n",
       " 'fundings_items_2_news_source': None,\n",
       " 'fundings_items_2_valuation': 10000000,\n",
       " 'fundings_items_2_valuation_generated_min': None,\n",
       " 'fundings_items_2_valuation_generated_max': None,\n",
       " 'fundings_items_3_id': 459763,\n",
       " 'fundings_items_3_year': 2019,\n",
       " 'fundings_items_3_month': 6,\n",
       " 'fundings_items_3_amount': 0.46,\n",
       " 'fundings_items_3_currency': 'CAD',\n",
       " 'fundings_items_3_round': 'ANGEL',\n",
       " 'fundings_items_3_investors': [],\n",
       " 'fundings_items_3_unknown_investors': [],\n",
       " 'fundings_items_3_is_undisclosed': False,\n",
       " 'fundings_items_3_news_source': None,\n",
       " 'fundings_items_3_valuation': 3330000,\n",
       " 'fundings_items_3_valuation_generated_min': None,\n",
       " 'fundings_items_3_valuation_generated_max': None,\n",
       " 'fundings_total': 4,\n",
       " 'growth_stage': 'seed',\n",
       " 'has_strong_founder': False,\n",
       " 'has_super_founder': False,\n",
       " 'has_promising_founder': False,\n",
       " 'hq_locations_0_id': 2636266,\n",
       " 'hq_locations_0_address': '1215 1 St SW, Calgary, AB T2R 0V3, Canada',\n",
       " 'hq_locations_0_street': '1 Street Southwest',\n",
       " 'hq_locations_0_street_number': '1215',\n",
       " 'hq_locations_0_zip': 'T2R 0V3',\n",
       " 'hq_locations_0_lat': 51.0408459,\n",
       " 'hq_locations_0_lon': -114.0660098,\n",
       " 'hq_locations_0_country_id': 147,\n",
       " 'hq_locations_0_country_name': 'Canada',\n",
       " 'hq_locations_0_country_lat': 56.130366,\n",
       " 'hq_locations_0_country_lon': -106.346771,\n",
       " 'hq_locations_0_country_trade_register_office': '',\n",
       " 'hq_locations_0_city_id': 2225,\n",
       " 'hq_locations_0_city_name': 'Calgary',\n",
       " 'hq_locations_0_city_lat': 51.048615,\n",
       " 'hq_locations_0_city_lon': -114.070846,\n",
       " 'hq_locations_0_continent_id': 8,\n",
       " 'hq_locations_0_continent_name': 'North America',\n",
       " 'hq_locations_0_is_headquarters': True,\n",
       " 'hq_locations_0_is_founding_location': True,\n",
       " 'hq_locations_0_trade_register_number': '',\n",
       " 'hq_locations_0_state_id': 24697,\n",
       " 'hq_locations_0_state_name': 'Alberta',\n",
       " 'hq_locations_0_state_lat': 1,\n",
       " 'hq_locations_0_state_lon': -115.002136,\n",
       " 'images_32x32': 'https://s3-eu-west-1.amazonaws.com/dealroom-images/52/MzI6MzI6Y29tcGFueUBzMy1ldS13ZXN0LTEuYW1hem9uYXdzLmNvbS9kZWFscm9vbS1pbWFnZXMvMjAyMS8wNy8yNS8zZDRhN2RlNTUxMWY0ODRjN2NhMjllMzIzOWY3MGMyMg==.jpg',\n",
       " 'images_74x74': 'https://s3-eu-west-1.amazonaws.com/dealroom-images/71/NzQ6NzQ6Y29tcGFueUBzMy1ldS13ZXN0LTEuYW1hem9uYXdzLmNvbS9kZWFscm9vbS1pbWFnZXMvMjAyMS8wNy8yNS8zZDRhN2RlNTUxMWY0ODRjN2NhMjllMzIzOWY3MGMyMg==.jpg',\n",
       " 'images_100x100': 'https://s3-eu-west-1.amazonaws.com/dealroom-images/0a/MTAwOjEwMDpjb21wYW55QHMzLWV1LXdlc3QtMS5hbWF6b25hd3MuY29tL2RlYWxyb29tLWltYWdlcy8yMDIxLzA3LzI1LzNkNGE3ZGU1NTExZjQ4NGM3Y2EyOWUzMjM5ZjcwYzIy.jpg',\n",
       " 'income_streams_0_id': 1,\n",
       " 'income_streams_0_name': 'advertising',\n",
       " 'income_streams_1_id': 2,\n",
       " 'income_streams_1_name': 'commission',\n",
       " 'income_streams_2_id': 3,\n",
       " 'income_streams_2_name': 'subscription',\n",
       " 'industries_0_id': 1264,\n",
       " 'industries_0_name': 'fintech',\n",
       " 'industries_1_id': 100147,\n",
       " 'industries_1_name': 'enterprise software',\n",
       " 'innovations': [],\n",
       " 'innovations_count': 0,\n",
       " 'investments_items': [],\n",
       " 'investments_total': 0,\n",
       " 'investors_items': [],\n",
       " 'investors_total': 0,\n",
       " 'is_editorial': False,\n",
       " 'is_ai_data': False,\n",
       " 'is_from_traderegister': False,\n",
       " 'latest_revenue_enhanced': None,\n",
       " 'latest_valuation_enhanced_id': 5130662,\n",
       " 'latest_valuation_enhanced_year': 2021,\n",
       " 'latest_valuation_enhanced_month': 7,\n",
       " 'latest_valuation_enhanced_day': None,\n",
       " 'latest_valuation_enhanced_source': 'kpi',\n",
       " 'latest_valuation_enhanced_source_round': None,\n",
       " 'latest_valuation_enhanced_valuation': 50000000,\n",
       " 'latest_valuation_enhanced_valuation_min': None,\n",
       " 'latest_valuation_enhanced_valuation_max': None,\n",
       " 'latest_valuation_enhanced_valuation_currency': 'CAD',\n",
       " 'latest_valuation_enhanced_market_cap': None,\n",
       " 'latest_valuation_enhanced_market_cap_currency': 'CAD',\n",
       " 'launch_month': 10,\n",
       " 'launch_year': 2019,\n",
       " 'linkedin_url': 'https://www.linkedin.com/company/heyauto/',\n",
       " 'lists_ids': [],\n",
       " 'matching_score': '0%',\n",
       " 'name': 'HeyAuto',\n",
       " 'participated_events_items': [],\n",
       " 'participated_events_total': 0,\n",
       " 'past_founders_raised_10m': 0,\n",
       " 'past_founders': 0,\n",
       " 'path': 'heyauto',\n",
       " 'playmarket_app_id': None,\n",
       " 'revenues_0_id': 832,\n",
       " 'revenues_0_name': 'marketplace & ecommerce',\n",
       " 'sdgs': [],\n",
       " 'service_industries': [],\n",
       " 'similarweb_12_months_growth_delta': None,\n",
       " 'similarweb_12_months_growth_percentile': None,\n",
       " 'similarweb_12_months_growth_relative': None,\n",
       " 'similarweb_12_months_growth_unique': None,\n",
       " 'similarweb_3_months_growth_delta': None,\n",
       " 'similarweb_3_months_growth_percentile': None,\n",
       " 'similarweb_3_months_growth_relative': None,\n",
       " 'similarweb_3_months_growth_unique': None,\n",
       " 'similarweb_6_months_growth_delta': None,\n",
       " 'similarweb_6_months_growth_percentile': None,\n",
       " 'similarweb_6_months_growth_relative': None,\n",
       " 'similarweb_6_months_growth_unique': None,\n",
       " 'similarweb_chart': [],\n",
       " 'sub_industries_0_id': 33,\n",
       " 'sub_industries_0_name': 'mortgages & lending',\n",
       " 'sub_industries_1_id': 35,\n",
       " 'sub_industries_1_name': 'insurance',\n",
       " 'tags_0_id': 2954,\n",
       " 'tags_0_name': 'analytics',\n",
       " 'tags_1_id': 11487,\n",
       " 'tags_1_name': 'big data &amp; analytics',\n",
       " 'tagline': 'The fastest growing auto marketplace in Canada!',\n",
       " 'technologies_0_id': 12,\n",
       " 'technologies_0_name': 'natural language processing',\n",
       " 'technologies_1_id': 7,\n",
       " 'technologies_1_name': 'machine learning',\n",
       " 'technologies_2_id': 10,\n",
       " 'technologies_2_name': 'deep learning',\n",
       " 'technologies_3_id': 5,\n",
       " 'technologies_3_name': 'big data',\n",
       " 'technologies_4_id': 2,\n",
       " 'technologies_4_name': 'artificial intelligence',\n",
       " 'total_funding': 0,\n",
       " 'total_jobs_available': 0,\n",
       " 'trading_multiples': None,\n",
       " 'type': 'company',\n",
       " 'tech_stack': [],\n",
       " 'twitter_url': 'https://twitter.com/heyautohq',\n",
       " 'job_roles': []}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = cleanup_record(companies_schema, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>angellist_url</th>\n",
       "      <th>appstore_app_id</th>\n",
       "      <th>client_focus_0_name</th>\n",
       "      <th>company_status</th>\n",
       "      <th>core_side_value</th>\n",
       "      <th>corporate_industries</th>\n",
       "      <th>create_date</th>\n",
       "      <th>crunchbase_url</th>\n",
       "      <th>employee_12_months_growth_delta</th>\n",
       "      <th>...</th>\n",
       "      <th>technologies_3_name</th>\n",
       "      <th>technologies_4_id</th>\n",
       "      <th>technologies_4_name</th>\n",
       "      <th>total_funding</th>\n",
       "      <th>total_jobs_available</th>\n",
       "      <th>trading_multiples</th>\n",
       "      <th>type</th>\n",
       "      <th>tech_stack</th>\n",
       "      <th>twitter_url</th>\n",
       "      <th>job_roles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3019447</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>consumer</td>\n",
       "      <td>operational</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-06-15T09:58:25+0100</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>big data</td>\n",
       "      <td>2</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>company</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/heyautohq</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id angellist_url appstore_app_id client_focus_0_name company_status  \\\n",
       "0  3019447          None            None            consumer    operational   \n",
       "\n",
       "  core_side_value corporate_industries               create_date  \\\n",
       "0            None                   []  2021-06-15T09:58:25+0100   \n",
       "\n",
       "  crunchbase_url employee_12_months_growth_delta  ... technologies_3_name  \\\n",
       "0           None                            None  ...            big data   \n",
       "\n",
       "  technologies_4_id      technologies_4_name total_funding  \\\n",
       "0                 2  artificial intelligence             0   \n",
       "\n",
       "  total_jobs_available trading_multiples     type tech_stack  \\\n",
       "0                    0              None  company         []   \n",
       "\n",
       "                     twitter_url job_roles  \n",
       "0  https://twitter.com/heyautohq        []  \n",
       "\n",
       "[1 rows x 206 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([nr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
